{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "airports = ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']\n",
    "\n",
    "# Set up master dataframe\n",
    "start_date = '2022-09-01 00:00:00'\n",
    "end_date = '2023-08-31 23:59:59'\n",
    "time_index = pd.date_range(start=start_date, end=end_date, freq='15min', inclusive='both')\n",
    "\n",
    "master_df = pd.DataFrame(index=time_index)\n",
    "master_df.index.name = 'prediction_time'\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "fuser_train = 'data/FUSER/FUSER_train'\n",
    "fuser_test = 'data/FUSER/FUSER_test'\n",
    "\n",
    "cwam_train = 'data/CWAM/CWAM_train'\n",
    "cwam_test = 'data/CWAM/CWAM_test'\n",
    "\n",
    "metar_train = 'data/METAR/METAR_train'\n",
    "metar_test = 'data/METAR/METAR_test'\n",
    "\n",
    "taf_train ='data/TAF/TAF_train'\n",
    "taf_test = 'data/TAF/TAF_test'\n",
    "\n",
    "data_out = 'cleaned_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fuser_files(pattern, airport):\n",
    "    files = []\n",
    "    for split in [fuser_train, fuser_test]:\n",
    "        for filename in os.listdir(os.path.join(split, airport)):\n",
    "            if len(filename) < len('KATL_2022-09-01_2022-10-02.runways_data_set.csv') and pattern in filename:\n",
    "                files.append(os.path.join(split, airport, filename))\n",
    "    return files\n",
    "\n",
    "def process_runways_data(airport):\n",
    "    files = get_fuser_files('runways_data_set', airport)\n",
    "    \n",
    "    # Concatenate all files for this airport into one combined_df\n",
    "    combined_df = pd.concat(\n",
    "        [pd.read_csv(f, parse_dates=['arrival_runway_actual_time', 'departure_runway_actual_time']) for f in files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Process arrivals\n",
    "    arrivals_df = (\n",
    "        combined_df[['gufi', 'arrival_runway_actual_time']]\n",
    "        .dropna()\n",
    "        .sort_values('arrival_runway_actual_time')\n",
    "        .drop_duplicates('gufi', keep='last')\n",
    "    )\n",
    "    \n",
    "    # Process departures\n",
    "    departures_df = (\n",
    "        combined_df[['gufi', 'departure_runway_actual_time']]\n",
    "        .dropna()\n",
    "        .sort_values('departure_runway_actual_time')\n",
    "        .drop_duplicates('gufi', keep='last')\n",
    "    )\n",
    "    \n",
    "    # Convert to time series\n",
    "    arrivals = pd.Series(1, index=arrivals_df['arrival_runway_actual_time'])\n",
    "    departures = pd.Series(1, index=departures_df['departure_runway_actual_time'])\n",
    "    \n",
    "    # Resample to 15-min intervals and count\n",
    "    arrival_counts = arrivals.resample('15min').count().rename(f'actual_arrivals_{airport}')\n",
    "    departure_counts = departures.resample('15min').count().rename(f'actual_departures_{airport}')\n",
    "    \n",
    "    counts_df = pd.concat([arrival_counts, departure_counts], axis=1).fillna(0)\n",
    "    \n",
    "    return counts_df\n",
    "\n",
    "    \n",
    "actual_arrivals_departures_df = pd.concat([process_runways_data(airport) for airport in airports], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_arrivals_departures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tbfm_data(airport):\n",
    "    print('Processing TBFM for ' + airport)\n",
    "    files = get_fuser_files('TBFM', airport)\n",
    "    \n",
    "    # Concatenate all files for this airport into one combined_df\n",
    "    combined_df = pd.concat(\n",
    "        [pd.read_csv(f, parse_dates=['arrival_runway_sta']) for f in files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Process arrivals\n",
    "    arrivals_df = (\n",
    "        combined_df[['gufi', 'arrival_runway_sta']]\n",
    "        .dropna(subset=['arrival_runway_sta'])\n",
    "        .drop_duplicates(subset='gufi', keep='last')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Convert to time series\n",
    "    arrivals = pd.Series(1, index=arrivals_df['arrival_runway_sta'])\n",
    "    \n",
    "    # Resample to 15-minute intervals and count\n",
    "    arrival_counts = arrivals.resample('15min').count().rename(f'scheduled_arrivals_{airport}')\n",
    "    \n",
    "    # Ensure the index is sorted before concatenation\n",
    "    arrival_counts = arrival_counts.sort_index()\n",
    "    counts_df = arrival_counts.to_frame().fillna(0)\n",
    "    \n",
    "    return counts_df\n",
    "    \n",
    "scheduled_arrivals_df = pd.concat([process_tbfm_data(airport) for airport in airports], axis=1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_arrivals_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_tfm_data(airport):\n",
    "    print('Processing TFM for ' + airport)\n",
    "    files = get_fuser_files('TFM', airport)\n",
    "    \n",
    "    # Concatenate all files for this airport into one combined_df\n",
    "    combined_df = pd.concat(\n",
    "        [pd.read_csv(f, parse_dates=['arrival_runway_estimated_time']) for f in files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Process arrivals\n",
    "    arrivals_df = (\n",
    "        combined_df[['gufi', 'arrival_runway_estimated_time']]\n",
    "        .dropna(subset=['arrival_runway_estimated_time'])\n",
    "        .drop_duplicates(subset='gufi', keep='last')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Convert to time series\n",
    "    arrivals = pd.Series(1, index=arrivals_df['arrival_runway_estimated_time'])\n",
    "    \n",
    "    # Resample to 15-minute intervals and count\n",
    "    arrival_counts = arrivals.resample('15min').count().rename(f'estimated_arrivals_{airport}')\n",
    "    \n",
    "    arrival_counts = arrival_counts.sort_index()\n",
    "    counts_df = arrival_counts.to_frame().fillna(0)\n",
    "    \n",
    "    return counts_df\n",
    "    \n",
    "estimated_arrivals_df = pd.concat([process_tfm_data(airport) for airport in airports], axis=1)    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_arrivals_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_etd_data(airport):\n",
    "    print('processing ETD for ' + airport)\n",
    "    files = get_fuser_files('ETD', airport)\n",
    "    \n",
    "    # Concatenate all files for this airport into one combined_df\n",
    "    combined_df = pd.concat(\n",
    "        [pd.read_csv(f, parse_dates=['departure_runway_estimated_time']) for f in files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Process departuress\n",
    "    departures_df = (\n",
    "        combined_df[['gufi', 'departure_runway_estimated_time']]\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset='gufi', keep='last')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Convert to time series\n",
    "    departures = pd.Series(1, index=departures_df['departure_runway_estimated_time'])\n",
    "    \n",
    "    # Resample to 15-minute intervals and count\n",
    "    departure_counts = departures.resample('15min').count().rename(f'estimated_departures_{airport}')\n",
    "    \n",
    "    departure_counts = departure_counts.sort_index()\n",
    "    counts_df = departure_counts.to_frame().fillna(0)\n",
    "    \n",
    "    return counts_df\n",
    "\n",
    "estimated_departures_df = pd.concat([process_etd_data(airport) for airport in airports], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_departures_df.head(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lamp_data(airport):\n",
    "    print('processing lamp data for ' + airport)\n",
    "    files = get_fuser_files('LAMP', airport)\n",
    "    \n",
    "    # Concatenate all files for this airport into one combined_df\n",
    "    combined_df = pd.concat(\n",
    "        [pd.read_csv(f, parse_dates=['forecast_timestamp']) for f in files],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Convert categorical columns\n",
    "    combined_df['cloud'] = pd.Categorical(combined_df['cloud']).codes\n",
    "    combined_df['lightning_prob'] = pd.Categorical(combined_df['lightning_prob']).codes\n",
    "    combined_df['precip'] = combined_df['precip'].astype(int)\n",
    "    \n",
    "    # Process featuers\n",
    "    columns_to_process = [\n",
    "        'temperature',\n",
    "        'wind_direction',\n",
    "        'wind_speed',\n",
    "        'wind_gust',\n",
    "        'cloud_ceiling',\n",
    "        'lightning_prob',\n",
    "        'precip'\n",
    "    ]\n",
    "    \n",
    "    processed_dfs = []\n",
    "    \n",
    "    for col in columns_to_process:\n",
    "        temp_df = combined_df[['forecast_timestamp', col]].drop_duplicates('forecast_timestamp', keep='last')\n",
    "        temp_df = temp_df.set_index('forecast_timestamp').sort_index()\n",
    "        \n",
    "        temp_df[col] = temp_df[col].astype(float)\n",
    "        \n",
    "        # Interpolate missing values\n",
    "        temp_df[col] = temp_df[col].interpolate(method='nearest')\n",
    "        \n",
    "        temp_df = temp_df.resample('15min').asfreq()\n",
    "        \n",
    "        temp_df[col] = temp_df[col].interpolate(method='nearest')\n",
    "        processed_dfs.append(temp_df[col].rename(f'{col}_{airport}'))\n",
    "      \n",
    "    if not processed_dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatenate all processed series\n",
    "    counts_df = pd.concat(processed_dfs, axis=1)\n",
    "    \n",
    "    return counts_df\n",
    "\n",
    "\n",
    "simple_weather_df = pd.concat([process_lamp_data(airport) for airport in airports], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_weather_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = [master_df, actual_arrivals_departures_df, scheduled_arrivals_df, estimated_arrivals_df, estimated_departures_df, simple_weather_df]\n",
    "\n",
    "# master_df = pd.concat(all_dfs, axis=1, join='outer')\n",
    "master_df = master_df.join(all_dfs, how = 'left')\n",
    "master_df = master_df.fillna(0)\n",
    "master_df = master_df[sorted(master_df.columns)]\n",
    "\n",
    "master_df.to_csv('all_fuser.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metar import Metar\n",
    "\n",
    "# Gather file paths\n",
    "files = []\n",
    "for file in os.listdir(metar_train):\n",
    "    filepath = os.path.join(metar_train, file)\n",
    "    files.append(filepath)\n",
    "\n",
    "for file in os.listdir(metar_test):\n",
    "    filepath = os.path.join(metar_test, file)\n",
    "    files.append(filepath)\n",
    "\n",
    "files.sort(key=lambda x: x[-17:])\n",
    "\n",
    "def format_weather_condition(weather_condition):\n",
    "    if isinstance(weather_condition, str):\n",
    "        return weather_condition\n",
    "    elif isinstance(weather_condition, tuple):\n",
    "        items = [str(item) for item in weather_condition if item is not None]\n",
    "        return ' '.join(items)\n",
    "    else:\n",
    "        return str(weather_condition)\n",
    "\n",
    "def format_sky_condition(sky_condition):\n",
    "    # Attempt to extract cover, height, cloud_type\n",
    "    if isinstance(sky_condition, tuple):\n",
    "        cover = sky_condition[0] if len(sky_condition) > 0 else None\n",
    "        height = sky_condition[1] if len(sky_condition) > 1 else None\n",
    "        cloud_type = sky_condition[2] if len(sky_condition) > 2 else None\n",
    "    else:\n",
    "        cover = getattr(sky_condition, 'cover', None)\n",
    "        height = getattr(sky_condition, 'height', None)\n",
    "        cloud_type = getattr(sky_condition, 'cloud_type', None)\n",
    "\n",
    "    parts = []\n",
    "    if cover:\n",
    "        parts.append(cover)\n",
    "    if height:\n",
    "        parts.append(str(height))\n",
    "    if cloud_type:\n",
    "        parts.append(cloud_type)\n",
    "    return ' '.join(parts)\n",
    "\n",
    "def parse_metar(metar_line, year, month):\n",
    "    try:\n",
    "        obs = Metar.Metar(metar_line, year=year, month=month, strict=False)\n",
    "    except Metar.ParserError as e:\n",
    "        print(f\"Error parsing METAR line: {metar_line}\\n{e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error parsing METAR line: {metar_line}\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    data = {\n",
    "        'station_id': obs.station_id,\n",
    "        'temp_c': obs.temp.value('C') if obs.temp else None,\n",
    "        'dewpt_c': obs.dewpt.value('C') if obs.dewpt else None,\n",
    "        'wind_dir_degrees': obs.wind_dir.value() if obs.wind_dir else None,\n",
    "        'wind_speed_kt': obs.wind_speed.value() if obs.wind_speed else None,\n",
    "        'wind_gust_kt': obs.wind_gust.value() if obs.wind_gust else None,\n",
    "        'visibility_m': obs.vis.value('M') if obs.vis else None,\n",
    "        'altimeter_hpa': obs.press.value('HPA') if obs.press else None,\n",
    "        'weather': ' '.join(format_weather_condition(w) for w in obs.weather) if obs.weather else '',\n",
    "        'clouds': ' '.join(format_sky_condition(c) for c in obs.sky) if obs.sky else '',\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def process_metar_data():\n",
    "    all_data = []\n",
    "\n",
    "    for idx, filepath in enumerate(files):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='ISO-8859-1', errors='replace') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            # Process lines in triplets: date line, metar line, blank line\n",
    "            for i in range(0, len(lines), 3):\n",
    "                if i + 1 >= len(lines):\n",
    "                    break\n",
    "                date_time_line = lines[i].strip()\n",
    "                metar_line = lines[i+1].strip()\n",
    "\n",
    "                if not metar_line:\n",
    "                    continue\n",
    "\n",
    "                first_token = metar_line.split(' ')[0] if metar_line else ''\n",
    "                if first_token not in airports:\n",
    "                    continue\n",
    "\n",
    "                # Parse observation_time\n",
    "                try:\n",
    "                    observation_time = datetime.strptime(date_time_line, \"%Y/%m/%d %H:%M\")\n",
    "                except ValueError:\n",
    "                    continue  # skip if datetime can't be parsed\n",
    "\n",
    "                # Parse the METAR line\n",
    "                year = int(date_time_line[:4])\n",
    "                month = int(date_time_line[5:7])\n",
    "                metar_data = parse_metar(metar_line, year, month)\n",
    "                if metar_data is None:\n",
    "                    continue\n",
    "\n",
    "                metar_data['observation_time'] = observation_time\n",
    "                all_data.append(metar_data)\n",
    "\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Error reading file {filepath}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing file {filepath}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No METAR data processed.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.DataFrame(all_data)\n",
    "    combined_df.set_index('observation_time', inplace=True)\n",
    "\n",
    "    # Remove duplicates\n",
    "    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
    "\n",
    "    # Factorize categorical columns\n",
    "    combined_df['weather_code'], _ = pd.factorize(combined_df['weather'])\n",
    "    combined_df['clouds_code'], _ = pd.factorize(combined_df['clouds'])\n",
    "    combined_df.drop(['weather', 'clouds'], axis=1, inplace=True)\n",
    "\n",
    "    # Pivot so each feature is separated by airport\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    pivoted = combined_df.pivot(index='observation_time', columns='station_id')\n",
    "\n",
    "    # Flatten columns\n",
    "    pivoted.columns = [f\"{feat}_{stn}\" for feat, stn in pivoted.columns]\n",
    "    pivoted.index = pd.to_datetime(pivoted.index)\n",
    "    pivoted.sort_index(inplace=True)\n",
    "\n",
    "    # Identify numeric and categorical code columns\n",
    "    categorical_cols = [col for col in pivoted.columns if 'weather_code' in col or 'clouds_code' in col]\n",
    "    numeric_cols = [col for col in pivoted.columns if col not in categorical_cols]\n",
    "\n",
    "    # Handle numeric data: Resample and interpolate\n",
    "    numeric_data = pivoted[numeric_cols].resample('15min').mean().interpolate(method='time')\n",
    "\n",
    "    categorical_data = pivoted[categorical_cols].resample('15min').ffill()\n",
    "\n",
    "    # Combine numeric and categorical\n",
    "    resampled = pd.concat([numeric_data, categorical_data], axis=1)\n",
    "\n",
    "    # For categorical codes, ensure they are integers. If NaN appears, fill with -1.\n",
    "    for col in categorical_cols:\n",
    "        resampled[col] = resampled[col].fillna(-1).astype(int)\n",
    "\n",
    "    return resampled\n",
    "\n",
    "metar_data = process_metar_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metar_data.columns = ['metar_'+ col for col in metar_data.columns]\n",
    "metar_data = metar_data[sorted(metar_data.columns)]\n",
    "\n",
    "metar_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df.join(metar_data, how = 'left')\n",
    "master_df = master_df.fillna(0)\n",
    "\n",
    "master_df.to_csv('fuser_and_metar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# Gather file paths\n",
    "files = []\n",
    "for file in os.listdir(taf_train):\n",
    "    filepath = os.path.join(taf_train, file)\n",
    "    files.append(filepath)\n",
    "\n",
    "for file in os.listdir(taf_test):\n",
    "    filepath = os.path.join(taf_test, file)\n",
    "    files.append(filepath)\n",
    "\n",
    "files.sort(key=lambda x: x[-17:])\n",
    "\n",
    "print(f\"Found {len(files)} TAF files to process\")\n",
    "\n",
    "\n",
    "def parse_taf_line(line):\n",
    "    \"\"\"Parse a single TAF line to extract relevant weather information.\"\"\"\n",
    "    # Initialize default values\n",
    "    data = {\n",
    "        'station': None,\n",
    "        'wind_dir': None, \n",
    "        'wind_speed': None,\n",
    "        'wind_gust': None,\n",
    "        'visibility': None,\n",
    "        'clouds': [],\n",
    "        'weather': None,\n",
    "        'valid_from': None,\n",
    "        'valid_to': None\n",
    "    }\n",
    "    \n",
    "    # Extract station ID (first 4 characters after 'TAF')\n",
    "    if 'TAF' in line:\n",
    "        parts = line.split()\n",
    "        try:\n",
    "            station_idx = parts.index('TAF') + 1\n",
    "            if station_idx < len(parts):\n",
    "                data['station'] = parts[station_idx][:4]\n",
    "        except ValueError:\n",
    "            # Handle case where 'TAF' is in line but not as a separate word\n",
    "            pass\n",
    "    else:\n",
    "        # Try to get first 4-letter code\n",
    "        match = re.search(r'\\b[A-Z]{4}\\b', line)\n",
    "        if match:\n",
    "            data['station'] = match.group()\n",
    "    \n",
    "    # Check if the extracted station is in the predefined list of airports\n",
    "    if data['station'] not in airports:\n",
    "        return None  # Skip this line if the station is not in the list\n",
    "    \n",
    "    # Extract date/time validity (format like 0106/0206)\n",
    "    validity = re.search(r'(\\d{4})/(\\d{4})', line)\n",
    "    if validity:\n",
    "        data['valid_from'] = validity.group(1)\n",
    "        data['valid_to'] = validity.group(2)\n",
    "    \n",
    "    # Extract wind information (format like 27010KT or 27010G15KT)\n",
    "    wind = re.search(r'(\\d{3}|VRB)(\\d{2,3})(?:G(\\d{2,3}))?KT', line)\n",
    "    if wind:\n",
    "        data['wind_dir'] = 0 if wind.group(1) == 'VRB' else int(wind.group(1))\n",
    "        data['wind_speed'] = int(wind.group(2))\n",
    "        data['wind_gust'] = int(wind.group(3)) if wind.group(3) else None\n",
    "    \n",
    "    # Extract visibility (format like 9999 or 3000)\n",
    "    vis = re.search(r'\\b(\\d{4})\\b(?!\\d)', line)\n",
    "    if vis:\n",
    "        data['visibility'] = int(vis.group(1))\n",
    "    \n",
    "    # Extract cloud information\n",
    "    cloud_pattern = r'(FEW|SCT|BKN|OVC)(\\d{3})(CB|TCU)?'\n",
    "    clouds = re.finditer(cloud_pattern, line)\n",
    "    for cloud in clouds:\n",
    "        data['clouds'].append({\n",
    "            'cover': cloud.group(1),\n",
    "            'height': int(cloud.group(2)) * 100,\n",
    "            'type': cloud.group(3) if cloud.group(3) else None\n",
    "        })\n",
    "    \n",
    "    # Extract weather phenomena\n",
    "    weather_pattern = r'\\b([-+]?)(?:RA|SN|TS|FG|BR|DZ|FU|HZ|SA|DU|SQ|FC|SS|DS)\\b'\n",
    "    weather = re.search(weather_pattern, line)\n",
    "    if weather:\n",
    "        data['weather'] = weather.group(0)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def process_taf_file(filepath):\n",
    "    \"\"\"Process a TAF file and return a DataFrame with 15-minute interval predictions.\"\"\"\n",
    "    all_forecasts = []\n",
    "    current_date = None\n",
    "    \n",
    "    with open(filepath, 'r', encoding='ISO-8859-1', errors='replace') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Parse date line\n",
    "        if re.match(r'\\d{4}/\\d{2}/\\d{2}', line):\n",
    "            current_date = datetime.strptime(line.split()[0], '%Y/%m/%d')\n",
    "            continue\n",
    "            \n",
    "        # Skip empty lines or lines without TAF data\n",
    "        if not line or not any(airport in line for airport in airports):\n",
    "            continue\n",
    "            \n",
    "        # Parse TAF line\n",
    "        taf_data = parse_taf_line(line)\n",
    "        \n",
    "        if not taf_data or not taf_data['station'] or not taf_data['valid_from'] or taf_data['station'] not in airports:\n",
    "            continue\n",
    "\n",
    "        # Convert hours >= 24 to next day(s)\n",
    "        from_hour = int(taf_data['valid_from'][:2])\n",
    "        from_days = from_hour // 24\n",
    "        from_hour = from_hour % 24\n",
    "        \n",
    "        to_hour = int(taf_data['valid_to'][:2]) \n",
    "        to_days = to_hour // 24\n",
    "        to_hour = to_hour % 24\n",
    "\n",
    "        valid_from = current_date.replace(\n",
    "            hour=from_hour,\n",
    "            minute=int(taf_data['valid_from'][2:])\n",
    "        ) + timedelta(days=from_days)\n",
    "\n",
    "        valid_to = current_date.replace(\n",
    "            hour=to_hour,\n",
    "            minute=int(taf_data['valid_to'][2:])\n",
    "        ) + timedelta(days=to_days)\n",
    "\n",
    "        # Handle case where valid_to is on next day\n",
    "        if valid_to < valid_from:\n",
    "            valid_to += timedelta(days=1)\n",
    "\n",
    "        # Generate 15-minute intervals\n",
    "        current_time = valid_from\n",
    "        while current_time <= valid_to:\n",
    "            forecast = {\n",
    "                'timestamp': current_time,\n",
    "                'station': taf_data['station'],\n",
    "                'wind_dir': taf_data['wind_dir'],\n",
    "                'wind_speed': taf_data['wind_speed'],\n",
    "                'wind_gust': taf_data['wind_gust'] if taf_data['wind_gust'] else 0,\n",
    "                'visibility': taf_data['visibility'],\n",
    "                'cloud_base': min([cloud['height'] for cloud in taf_data['clouds']]) if taf_data['clouds'] else None,\n",
    "                'weather_code': hash(taf_data['weather']) % 100 if taf_data['weather'] else 0\n",
    "            }\n",
    "            all_forecasts.append(forecast)\n",
    "            current_time += timedelta(minutes=15)\n",
    "    \n",
    "    if not all_forecasts:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_forecasts)\n",
    "    \n",
    "    # Pivot the data so each feature is separated by airport, dropping duplicates first\n",
    "    df = df.drop_duplicates(subset=['timestamp', 'station'], keep='last')\n",
    "    df = df.pivot(index='timestamp', columns='station')\n",
    "    \n",
    "    # Flatten column names\n",
    "    df.columns = [f'taf_{feat}_{stn}' for feat, stn in df.columns]\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Forward fill missing values within each forecast period (6 hours)\n",
    "    df = df.ffill(limit=24)  # 24 periods = 6 hours\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process all TAF files\n",
    "all_taf_data = []\n",
    "processed_count = 0\n",
    "print(\"Processing TAF files...\")\n",
    "\n",
    "for filepath in files:\n",
    "    taf_df = process_taf_file(filepath)\n",
    "    if not taf_df.empty:\n",
    "        all_taf_data.append(taf_df)\n",
    "        processed_count += 1\n",
    "        if processed_count % 10 == 0:  # Log every 10 files\n",
    "            print(f\"Processed {processed_count}/{len(files)} files\")\n",
    "\n",
    "print(f\"\\nSuccessfully processed {processed_count} files\")\n",
    "\n",
    "# Combine all TAF data\n",
    "if all_taf_data:\n",
    "    print(\"Combining all TAF data...\")\n",
    "    taf_data = pd.concat(all_taf_data)\n",
    "    taf_data = taf_data[~taf_data.index.duplicated(keep='last')]\n",
    "    taf_data.sort_index(inplace=True)\n",
    "    \n",
    "    # Resample to ensure consistent 15-minute intervals\n",
    "    taf_data = taf_data.resample('15min').ffill()\n",
    "    print(\"TAF data processing complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort columns and index\n",
    "taf_data.fillna(0)\n",
    "taf_data = taf_data[sorted(taf_data.columns)]\n",
    "\n",
    "master_df = pd.read_csv('cleaned_data/fuser_and_metar.csv')\n",
    "master_df['prediction_time'] = pd.to_datetime(master_df['prediction_time'])\n",
    "master_df.set_index('prediction_time', inplace=True)\n",
    "\n",
    "master_df = master_df.join(taf_data, how='left')\n",
    "master_df = master_df.fillna(0)\n",
    "\n",
    "master_df.to_csv('fuser_metar_taf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def add_time_features(df):\n",
    "    \"\"\"Add time-based features\"\"\"\n",
    "    # Convert index to datetime if needed\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Basic time features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['month'] = df.index.month\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Peak travel times (rough approximations)\n",
    "    df['is_morning_peak'] = ((df['hour'] >= 6) & (df['hour'] <= 9)).astype(int)\n",
    "    df['is_evening_peak'] = ((df['hour'] >= 16) & (df['hour'] <= 19)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_features(df):\n",
    "    \"\"\"Add rolling window features\"\"\"\n",
    "    windows = [4, 12, 24]  # 1 hour, 3 hours, 6 hours (assuming 15-min intervals)\n",
    "    \n",
    "    for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']:\n",
    "        for window in windows:\n",
    "            # Rolling means for actual arrivals/departures\n",
    "            df[f'rolling_{window}p_arrivals_{airport}'] = df[f'actual_arrivals_{airport}'].rolling(window=window).mean()\n",
    "            df[f'rolling_{window}p_departures_{airport}'] = df[f'actual_departures_{airport}'].rolling(window=window).mean()\n",
    "            \n",
    "            # Rolling max for weather severity\n",
    "            df[f'rolling_{window}p_weather_severity_{airport}'] = df[f'weather_severity_{airport}'].rolling(window=window).max()\n",
    "            \n",
    "            # Rolling standard deviation for arrivals/departures\n",
    "            df[f'rolling_{window}p_arrivals_std_{airport}'] = df[f'actual_arrivals_{airport}'].rolling(window=window).std()\n",
    "            df[f'rolling_{window}p_departures_std_{airport}'] = df[f'actual_departures_{airport}'].rolling(window=window).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_lag_features(df):\n",
    "    \"\"\"Add lagged features\"\"\"\n",
    "    lags = [1, 2, 4, 8]  # 15min, 30min, 1hr, 2hr\n",
    "    \n",
    "    for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']:\n",
    "        for lag in lags:\n",
    "            # Lag actual arrivals/departures\n",
    "            df[f'lag_{lag}p_arrivals_{airport}'] = df[f'actual_arrivals_{airport}'].shift(lag)\n",
    "            df[f'lag_{lag}p_departures_{airport}'] = df[f'actual_departures_{airport}'].shift(lag)\n",
    "            \n",
    "            # Lag weather features\n",
    "            df[f'lag_{lag}p_weather_severity_{airport}'] = df[f'weather_severity_{airport}'].shift(lag)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_differential_features(df):\n",
    "    \"\"\"Add rate of change features\"\"\"\n",
    "    for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']:\n",
    "        # Rate of change for arrivals/departures\n",
    "        df[f'arrivals_change_{airport}'] = df[f'actual_arrivals_{airport}'].diff()\n",
    "        df[f'departures_change_{airport}'] = df[f'actual_departures_{airport}'].diff()\n",
    "        \n",
    "        # Rate of change for weather severity\n",
    "        df[f'weather_severity_change_{airport}'] = df[f'weather_severity_{airport}'].diff()\n",
    "        \n",
    "        # Acceleration (second derivative) of arrivals/departures\n",
    "        df[f'arrivals_acceleration_{airport}'] = df[f'arrivals_change_{airport}'].diff()\n",
    "        df[f'departures_acceleration_{airport}'] = df[f'departures_change_{airport}'].diff()\n",
    "    \n",
    "    return df\n",
    "# Add these new functions after the existing ones\n",
    "\n",
    "def add_taf_vs_metar_features(df):\n",
    "    \"\"\"Add features comparing TAF forecasts with METAR actuals\"\"\"\n",
    "    for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']:\n",
    "        # Wind direction difference\n",
    "        df[f'wind_dir_forecast_error_{airport}'] = np.abs(\n",
    "            df[f'taf_wind_dir_{airport}'] - df[f'metar_wind_dir_degrees_{airport}']\n",
    "        ).clip(0, 180)  # Max difference is 180 degrees\n",
    "        \n",
    "        # Wind speed difference\n",
    "        df[f'wind_speed_forecast_error_{airport}'] = np.abs(\n",
    "            df[f'taf_wind_speed_{airport}'] - df[f'metar_wind_speed_kt_{airport}']\n",
    "        )\n",
    "        \n",
    "        # Visibility difference\n",
    "        df[f'visibility_forecast_error_{airport}'] = np.abs(\n",
    "            df[f'taf_visibility_{airport}'] - df[f'metar_visibility_m_{airport}']\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_network_features(df):\n",
    "    \"\"\"Add features capturing network effects between airports\"\"\"\n",
    "    windows = [4, 12, 24]  # 1, 3, 6 hours\n",
    "    \n",
    "    # Calculate total system load\n",
    "    df['total_system_arrivals'] = sum(df[f'actual_arrivals_{airport}'] \n",
    "                                    for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', \n",
    "                                                  'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA'])\n",
    "    df['total_system_departures'] = sum(df[f'actual_departures_{airport}'] \n",
    "                                      for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', \n",
    "                                                    'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA'])\n",
    "    \n",
    "    # Calculate rolling system metrics\n",
    "    for window in windows:\n",
    "        df[f'rolling_{window}p_system_arrivals'] = df['total_system_arrivals'].rolling(window=window).mean()\n",
    "        df[f'rolling_{window}p_system_departures'] = df['total_system_departures'].rolling(window=window).mean()\n",
    "        \n",
    "        # System-wide weather severity\n",
    "        df[f'airports_with_severe_weather_{window}p'] = sum(\n",
    "            (df[f'weather_severity_{airport}'] > 2).rolling(window=window).max()\n",
    "            for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', \n",
    "                          'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_capacity_utilization_features(df):\n",
    "    \"\"\"Add features related to airport capacity utilization\"\"\"\n",
    "    for airport in ['KATL', 'KCLT', 'KDEN', 'KDFW', 'KJFK', 'KMEM', 'KMIA', 'KORD', 'KPHX', 'KSEA']:\n",
    "        # Total operations per 15-min period\n",
    "        df[f'total_operations_{airport}'] = (\n",
    "            df[f'actual_arrivals_{airport}'] + df[f'actual_departures_{airport}']\n",
    "        )\n",
    "        \n",
    "        # Rolling 1-hour operation counts\n",
    "        df[f'rolling_1h_operations_{airport}'] = df[f'total_operations_{airport}'].rolling(4).sum()\n",
    "        \n",
    "        # Capacity pressure (ratio of current operations to rolling max)\n",
    "        rolling_max = df[f'total_operations_{airport}'].rolling(96).max()  # 24-hour rolling max\n",
    "        df[f'capacity_pressure_{airport}'] = (\n",
    "            df[f'total_operations_{airport}'] / rolling_max.replace(0, 1)\n",
    "        ).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_seasonal_features(df):\n",
    "    \"\"\"Add seasonal and cyclical features\"\"\"\n",
    "    # Time of day as cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    \n",
    "    # Day of week as cyclical features\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Month as cyclical features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Modify the main function to include new features\n",
    "def main():\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv('fuser_metar_taf.csv', index_col=0, parse_dates=True)\n",
    "    \n",
    "    print(\"Adding time features...\")\n",
    "    df = add_time_features(df)\n",
    "    \n",
    "    print(\"Adding TAF vs METAR comparison features...\")\n",
    "    df = add_taf_vs_metar_features(df)\n",
    "    \n",
    "    print(\"Adding network features...\")\n",
    "    df = add_network_features(df)\n",
    "    \n",
    "    print(\"Adding capacity utilization features...\")\n",
    "    df = add_capacity_utilization_features(df)\n",
    "    \n",
    "    print(\"Adding seasonal features...\")\n",
    "    df = add_seasonal_features(df)\n",
    "    \n",
    "    print(\"Adding rolling features...\")\n",
    "    df = add_rolling_features(df)\n",
    "    \n",
    "    print(\"Adding lag features...\")\n",
    "    df = add_lag_features(df)\n",
    "    \n",
    "    print(\"Adding differential features...\")\n",
    "    df = add_differential_features(df)\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    print(\"Saving enhanced dataset...\")\n",
    "    df.to_csv('fuser_metar_taf_enhanced.csv')\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_cwam_files():\n",
    "    cwam_files = []\n",
    "    \n",
    "    # Walk through both train and test directories\n",
    "    print(\"Searching for CWAM files in train directory...\")\n",
    "    for root, dirs, files in os.walk(cwam_train):\n",
    "        for file in files:\n",
    "            if file.endswith('.bz2'):  # Only get the CWAM files\n",
    "                cwam_files.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "                \n",
    "    print(\"\\nSearching for CWAM files in test directory...\")        \n",
    "    for root, dirs, files in os.walk(cwam_test):\n",
    "        for file in files:\n",
    "            if file.endswith('.bz2'):\n",
    "                cwam_files.append(os.path.join(root, file))\n",
    "                \n",
    "    \n",
    "    return sorted(cwam_files)\n",
    "\n",
    "cwam_files = get_cwam_files()\n",
    "print(f\"\\nTotal CWAM files found: {len(cwam_files)}\")\n",
    "\n",
    "# Print first few files to verify\n",
    "print(\"\\nSample files:\")\n",
    "for file in cwam_files[:5]:\n",
    "    print(file)\n",
    "\n",
    "import h5py\n",
    "import bz2\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "\n",
    "def extract_timestamp(filename):\n",
    "    # Extract timestamp from filename like 2022_12_18_23_45_GMT.Forecast.h5.CWAM.h5.bz2\n",
    "    basename = os.path.basename(filename)\n",
    "    date_part = basename.split('_GMT')[0]\n",
    "    return datetime.strptime(date_part, '%Y_%m_%d_%H_%M')\n",
    "\n",
    "def point_in_polygon(point, polygon_coords):\n",
    "    \"\"\"Check if point falls within polygon\"\"\"\n",
    "    point = Point(point)\n",
    "    polygon = Polygon(polygon_coords)\n",
    "    return polygon.contains(point)\n",
    "\n",
    "# Airport coordinates (lat, lon)\n",
    "airport_coords = {\n",
    "    'KATL': (33.6367, -84.4281),\n",
    "    'KCLT': (35.2141, -80.9431),\n",
    "    'KDEN': (39.8561, -104.6737),\n",
    "    'KDFW': (32.8972, -97.0376),\n",
    "    'KJFK': (40.6413, -73.7781),\n",
    "    'KMEM': (35.0424, -89.9767),\n",
    "    'KMIA': (25.7932, -80.2906),\n",
    "    'KORD': (41.9786, -87.9048),\n",
    "    'KPHX': (33.4342, -112.0117),\n",
    "    'KSEA': (47.4502, -122.3088)\n",
    "}\n",
    "\n",
    "import h5py\n",
    "import bz2\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "def process_cwam_file(filepath):\n",
    "    \"\"\"Process single CWAM file and return features for each airport\"\"\"\n",
    "    timestamp = extract_timestamp(filepath)\n",
    "    features = {}\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile() as temp_file:\n",
    "        with bz2.open(filepath, 'rb') as f_in:\n",
    "            temp_file.write(f_in.read())\n",
    "            temp_file.flush()\n",
    "            \n",
    "        with h5py.File(temp_file.name, 'r') as f:\n",
    "            # Process each forecast time (0 to 120 minutes, 5-minute intervals)\n",
    "            forecast_times = [g for g in f['Deviation Probability'].keys() if g.startswith('FCST')]\n",
    "            \n",
    "            # Initialize features for each airport\n",
    "            for airport in airport_coords:\n",
    "                features[airport] = {\n",
    "                    'timestamp': timestamp,\n",
    "                    # Current conditions (FCST000)\n",
    "                    'current_impact_60': 0,  # Binary: is airport currently impacted at 60% threshold?\n",
    "                    'current_impact_70': 0,\n",
    "                    'current_impact_80': 0,\n",
    "                    'current_distance_60': float('inf'),  # Distance to nearest polygon if not impacted\n",
    "                    'current_distance_70': float('inf'),\n",
    "                    'current_distance_80': float('inf'),\n",
    "                    # Short-term forecast (next 30 minutes)\n",
    "                    'short_term_impacts': 0,  # Number of 5-min periods with impact\n",
    "                    'time_to_impact': float('inf'),  # Minutes until first impact\n",
    "                    # Medium-term forecast (30-120 minutes)\n",
    "                    'medium_term_impacts': 0,\n",
    "                    'severe_weather_approaching': 0  # Binary: weather moving towards airport\n",
    "                }\n",
    "            \n",
    "            # Process each forecast time\n",
    "            for fcst in sorted(forecast_times):\n",
    "                minutes = int(fcst[4:])  # Extract minutes from FCST000\n",
    "                base_path = f'Deviation Probability/{fcst}/FLVL250/Contour'\n",
    "                \n",
    "                for airport, coords in airport_coords.items():\n",
    "                    airport_point = Point(coords)\n",
    "                    \n",
    "                    # Process each threshold\n",
    "                    for thresh in ['060', '070', '080']:\n",
    "                        thresh_path = f'{base_path}/TRSH{thresh}'\n",
    "                        if thresh_path not in f:\n",
    "                            continue\n",
    "                            \n",
    "                        group = f[thresh_path]\n",
    "                        is_impacted = False\n",
    "                        min_distance = float('inf')\n",
    "                        \n",
    "                        # Check all polygons\n",
    "                        for poly_name in group.keys():\n",
    "                            if not poly_name.startswith('POLY'):\n",
    "                                continue\n",
    "                                \n",
    "                            poly_coords = group[poly_name][:]\n",
    "                            poly_coords = poly_coords.reshape(-1, 2)\n",
    "                            polygon = Polygon(poly_coords)\n",
    "                            \n",
    "                            if polygon.contains(airport_point):\n",
    "                                is_impacted = True\n",
    "                                min_distance = 0\n",
    "                                break\n",
    "                            else:\n",
    "                                min_distance = min(min_distance, airport_point.distance(polygon))\n",
    "                        \n",
    "                        # Update features based on forecast time\n",
    "                        if minutes == 0:  # Current conditions\n",
    "                            features[airport][f'current_impact_{thresh[-2:]}'] = int(is_impacted)\n",
    "                            features[airport][f'current_distance_{thresh[-2:]}'] = min_distance\n",
    "                        elif minutes <= 30:  # Short-term forecast\n",
    "                            if is_impacted:\n",
    "                                features[airport]['short_term_impacts'] += 1\n",
    "                                if features[airport]['time_to_impact'] == float('inf'):\n",
    "                                    features[airport]['time_to_impact'] = minutes\n",
    "                        else:  # Medium-term forecast\n",
    "                            if is_impacted:\n",
    "                                features[airport]['medium_term_impacts'] += 1\n",
    "                                \n",
    "                        # Check if weather is approaching airport\n",
    "                        if minutes > 0 and min_distance < features[airport][f'current_distance_{thresh[-2:]}']:\n",
    "                            features[airport]['severe_weather_approaching'] = 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Process files and create DataFrame\n",
    "print(\"\\nStarting CWAM file processing...\")\n",
    "all_features = []\n",
    "\n",
    "for i, filepath in enumerate(cwam_files):\n",
    "    try:\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\n Proccesing File {i} / {len(cwam_files)}\")\n",
    "        features = process_cwam_file(filepath)\n",
    "        all_features.append(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame with airport-specific column names\n",
    "rows = []\n",
    "for features in all_features:\n",
    "    for airport, data in features.items():\n",
    "        row = {\n",
    "            'timestamp': data['timestamp'],\n",
    "            f'cwam_current_impact_60_{airport}': data['current_impact_60'],\n",
    "            f'cwam_current_impact_70_{airport}': data['current_impact_70'],\n",
    "            f'cwam_current_impact_80_{airport}': data['current_impact_80'],\n",
    "            f'cwam_distance_to_weather_60_{airport}': data['current_distance_60'],\n",
    "            f'cwam_distance_to_weather_70_{airport}': data['current_distance_70'],\n",
    "            f'cwam_distance_to_weather_80_{airport}': data['current_distance_80'],\n",
    "            f'cwam_impacts_next_30min_{airport}': data['short_term_impacts'],\n",
    "            f'cwam_minutes_until_impact_{airport}': data['time_to_impact'],\n",
    "            f'cwam_impacts_30_120min_{airport}': data['medium_term_impacts'],\n",
    "            f'cwam_weather_approaching_{airport}': data['severe_weather_approaching']\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "cwam_df = pd.DataFrame(rows)\n",
    "cwam_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Sort columns alphabetically\n",
    "cwam_df = cwam_df[sorted(cwam_df.columns)]\n",
    "\n",
    "# Resample to 15-minute intervals and forward fill\n",
    "cwam_df = cwam_df.resample('15min').ffill()\n",
    "\n",
    "# Save to CSV\n",
    "cwam_df.to_csv('cwam_features_sample.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
